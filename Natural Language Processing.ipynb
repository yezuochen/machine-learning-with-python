{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing with RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map word to integer representing it\n",
    "vocab = {}\n",
    "word_encoding = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(text):\n",
    "    global word_encoding\n",
    "\n",
    "    # create a list of all of the word\n",
    "    words = text.lower().split(\" \")\n",
    "    # stores all of the encodings and their frequency\n",
    "    bag = {}\n",
    "\n",
    "    for word in words:\n",
    "        if word in vocab:\n",
    "            # get encoding from vocab\n",
    "            encoding = vocab[word]\n",
    "        else:\n",
    "            vocab[word] = word_encoding\n",
    "            word_encoding += 1\n",
    "\n",
    "        if encoding in bag:\n",
    "            bag[encoding] += 1\n",
    "        else:\n",
    "            bag[encoding] = 1\n",
    "    \n",
    "    return bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 2, 2: 3, 3: 3, 4: 3, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1}\n",
      "{'this': 1, 'is': 2, 'a': 3, 'test': 4, 'to': 5, 'see': 6, 'if': 7, 'will': 8, 'work': 9}\n"
     ]
    }
   ],
   "source": [
    "text = \"this is a test to see if this test will work is is test a a\"\n",
    "bag = bag_of_words(text)\n",
    "print(bag)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movie Review Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\zuoch\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "import tensorflow as tf \n",
    "import os\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 88584\n",
    "MAXLEN = 250\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sequence.pad_sequences(train_data, MAXLEN)\n",
    "test_data = sequence.pad_sequences(test_data, MAXLEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\zuoch\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(VOCAB_SIZE, 32),\n",
    "    tf.keras.layers.LSTM(32),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 32)          2834688   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 32)                8320      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2843041 (10.85 MB)\n",
      "Trainable params: 2843041 (10.85 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\zuoch\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:From c:\\Users\\zuoch\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\zuoch\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "625/625 [==============================] - 39s 58ms/step - loss: 0.4498 - acc: 0.7808 - val_loss: 0.3831 - val_acc: 0.8514\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 32s 52ms/step - loss: 0.2580 - acc: 0.9004 - val_loss: 0.2815 - val_acc: 0.8864\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 32s 52ms/step - loss: 0.1980 - acc: 0.9270 - val_loss: 0.3736 - val_acc: 0.8750\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 33s 53ms/step - loss: 0.1605 - acc: 0.9448 - val_loss: 0.2872 - val_acc: 0.8842\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 33s 53ms/step - loss: 0.1352 - acc: 0.9531 - val_loss: 0.3332 - val_acc: 0.8838\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 35s 56ms/step - loss: 0.1125 - acc: 0.9629 - val_loss: 0.3674 - val_acc: 0.8810\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 31s 49ms/step - loss: 0.0936 - acc: 0.9696 - val_loss: 0.3567 - val_acc: 0.8850\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 34s 54ms/step - loss: 0.0780 - acc: 0.9753 - val_loss: 0.4070 - val_acc: 0.8724\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 34s 54ms/step - loss: 0.0678 - acc: 0.9793 - val_loss: 0.3899 - val_acc: 0.8774\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 34s 55ms/step - loss: 0.0588 - acc: 0.9823 - val_loss: 0.4240 - val_acc: 0.8728\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_data, train_labels, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 12s 16ms/step - loss: 0.4726 - acc: 0.8608\n",
      "[0.4726109802722931, 0.86080002784729]\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(test_data, test_labels)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(text):\n",
    "    tokens = keras.preprocessing.text.text_to_word_sequence(text)\n",
    "    tokens = [word_index[word] if word in word_index else 0 for word in tokens]\n",
    "    return sequence.pad_sequences([tokens], MAXLEN)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0  12  17  13  40 477  35 477]\n"
     ]
    }
   ],
   "source": [
    "text = \"that movie was just amazing, so amazing\"\n",
    "encoded = encode_text(text)\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# while were at it lets a decode function\n",
    "reverse_word_index = {value: key for (key, value) in word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_integers(integers):\n",
    "    PAD = 0\n",
    "    text = \"\"\n",
    "    for num in integers:\n",
    "        if num != PAD:\n",
    "            text += reverse_word_index[num] + \" \"\n",
    "        \n",
    "    return text[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "that movie was just amazing so amazing\n"
     ]
    }
   ],
   "source": [
    "print(decode_integers(encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction\n",
    "def predict(text):\n",
    "    encoded_text = encode_text(text)\n",
    "    pred = np.zeros((1, 250))\n",
    "    pred[0] = encoded_text\n",
    "    result = model.predict(pred)\n",
    "    print(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 650ms/step\n",
      "[0.8943222]\n"
     ]
    }
   ],
   "source": [
    "positive_review = \"That movie was so awesome! I really loved it and would watch it again because it was amazingly great\"\n",
    "predict(positive_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 33ms/step\n",
      "[0.22685647]\n"
     ]
    }
   ],
   "source": [
    "negative_review = \"that movie sucked. I hated it and wouldn't watch it again. Was one of the worst things I've ever watched\"\n",
    "predict(negative_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Play Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
      "1115394/1115394 [==============================] - 1s 1us/step\n"
     ]
    }
   ],
   "source": [
    "path_to_file = tf.keras.utils.get_file(\"shakespeare.txt\", \"https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Own Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import files\n",
    "# path_to_file = list(files.upload().keys())[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Contents of File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "# read, then decode for py2 compat\n",
    "text = open(path_to_file, \"rb\").read().decode(encoding=\"utf-8\")\n",
    "\n",
    "# length of text is the number of characters in it\n",
    "print(\"Length of text: {} characters\".format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### first 250 characters in text\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(set(text))\n",
    "\n",
    "# creating a mapping from unique characters to indices\n",
    "char2idx = {u: i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_int(text):\n",
    "    return np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_as_int = text_to_int(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:  First Citizen\n",
      "Encoded:  [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
     ]
    }
   ],
   "source": [
    "# how part of our text is encoded\n",
    "print(\"Text: \", text[:13])\n",
    "print(\"Encoded: \", text_to_int(text[:13]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_to_text(ints):\n",
    "    try:\n",
    "        ints = ints.numpy()\n",
    "    except:\n",
    "        pass\n",
    "    return \"\".join(idx2char[ints])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen\n"
     ]
    }
   ],
   "source": [
    "print(int_to_text(text_as_int[:13]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Training Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# length of sequence for a training example\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text) // (seq_length+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "EXAMPLE\n",
      "\n",
      "INPUT\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n",
      "\n",
      "OUTPUT\n",
      "irst Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You \n",
      "\n",
      "\n",
      "EXAMPLE\n",
      "\n",
      "INPUT\n",
      "are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you \n",
      "\n",
      "OUTPUT\n",
      "re all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you k\n"
     ]
    }
   ],
   "source": [
    "for x, y in dataset.take(2):\n",
    "    print(\"\\n\\nEXAMPLE\\n\")\n",
    "    print(\"INPUT\")\n",
    "    print(int_to_text(x))\n",
    "    print(\"\\nOUTPUT\")\n",
    "    print(int_to_text(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "# vocab is number of unique characters\n",
    "VOCAB_SIZE = len(vocab)\n",
    "EMBEDDING_DIM = 256\n",
    "RNN_UNITS = 1024\n",
    "# buffer size to shuffle the dataset\n",
    "# TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory, \n",
    "# Instead, it maintains a buffer in which it shuffles elements.\n",
    "BUFFER_SIZE = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                                  batch_input_shape=[batch_size, None]),\n",
    "        tf.keras.layers.LSTM(rnn_units,\n",
    "                             return_sequences=True,\n",
    "                             stateful=True,\n",
    "                             recurrent_initializer=\"glorot_uniform\"),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (64, None, 256)           16640     \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (64, None, 1024)          5246976   \n",
      "                                                                 \n",
      " dense_2 (Dense)             (64, None, 65)            66625     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5330241 (20.33 MB)\n",
      "Trainable params: 5330241 (20.33 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in data.take(1):\n",
    "    # ask model for a prediction on our first batch of training data\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "tf.Tensor(\n",
      "[[-0.00931592 -0.00268863  0.01062842 ...  0.0154992   0.00932966\n",
      "  -0.00160682]\n",
      " [-0.00425422 -0.00059597  0.01095393 ...  0.0221952   0.00658926\n",
      "  -0.0034372 ]\n",
      " [-0.00948711 -0.00014567  0.01100174 ...  0.02243002  0.00426236\n",
      "  -0.0041007 ]\n",
      " ...\n",
      " [-0.00021257  0.00157258  0.0132362  ...  0.00955123  0.01543389\n",
      "  -0.00019209]\n",
      " [ 0.00708039  0.00103649  0.01067642 ...  0.01118428  0.01162604\n",
      "  -0.00123282]\n",
      " [ 0.01053786 -0.00408195  0.01412772 ...  0.0094512   0.00699033\n",
      "   0.00119845]], shape=(100, 65), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# the prediction is an array of 64 arrays, one for each entry in the batch\n",
    "print(len(example_batch_predictions))\n",
    "print(example_batch_predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "tf.Tensor(\n",
      "[[-0.00931592 -0.00268863  0.01062842 ...  0.0154992   0.00932966\n",
      "  -0.00160682]\n",
      " [-0.00425422 -0.00059597  0.01095393 ...  0.0221952   0.00658926\n",
      "  -0.0034372 ]\n",
      " [-0.00948711 -0.00014567  0.01100174 ...  0.02243002  0.00426236\n",
      "  -0.0041007 ]\n",
      " ...\n",
      " [-0.00021257  0.00157258  0.0132362  ...  0.00955123  0.01543389\n",
      "  -0.00019209]\n",
      " [ 0.00708039  0.00103649  0.01067642 ...  0.01118428  0.01162604\n",
      "  -0.00123282]\n",
      " [ 0.01053786 -0.00408195  0.01412772 ...  0.0094512   0.00699033\n",
      "   0.00119845]], shape=(100, 65), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# examine one prediction\n",
    "pred = example_batch_predictions[0]\n",
    "print(len(pred))\n",
    "print(pred)\n",
    "\n",
    "# this is a 2d array of length 100, each interior array is the prediction for the next character at each time step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n",
      "tf.Tensor(\n",
      "[-9.3159219e-03 -2.6886256e-03  1.0628422e-02 -7.1298564e-04\n",
      "  3.0778358e-03 -5.1899427e-03 -2.4528732e-03  2.6836926e-03\n",
      "  6.1333016e-03 -8.0910586e-03 -5.4772384e-03 -5.3892252e-03\n",
      " -7.8792004e-03 -5.0856406e-04  2.9113153e-03  4.1922904e-04\n",
      "  1.4595806e-03 -8.5873324e-03 -8.0247065e-03 -5.7862252e-03\n",
      "  7.2533083e-03 -2.0340579e-03  2.9454394e-03  2.4055922e-03\n",
      " -4.3149577e-03  8.2649309e-03 -2.9980845e-03 -4.9899155e-03\n",
      "  4.1082567e-03 -2.4163662e-03 -3.9016004e-03 -2.4290432e-03\n",
      " -8.2970597e-03  8.0897473e-05  1.4212692e-02 -1.2347613e-03\n",
      " -2.0645615e-03  1.0640031e-02 -2.2951285e-03  5.9195622e-03\n",
      "  5.4743048e-03  3.2921140e-03  3.2425835e-04  1.0469848e-02\n",
      "  6.9688726e-04 -5.6243897e-03  6.5230643e-03  5.1852567e-03\n",
      " -1.1978524e-03 -8.1989970e-03 -5.9228875e-03  6.8507353e-03\n",
      " -1.4947127e-03 -6.3539166e-03 -1.3106934e-02 -2.2907569e-03\n",
      "  2.8492277e-03 -6.0464684e-03  7.5841090e-06  1.1155365e-02\n",
      " -7.1356702e-04  4.0797540e-03  1.5499196e-02  9.3296617e-03\n",
      " -1.6068178e-03], shape=(65,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# a prediction at the first timestep\n",
    "time_pred = pred[0]\n",
    "print(len(time_pred))\n",
    "print(time_pred)\n",
    "\n",
    "# its 65 values representing the probability of each character occuring next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BNLd3Y, IaRf;;\n",
      "z$eLNVthfcgBHzliEbphc3lPMYGGD$rftaHSsbIJq;ZNRaPG qokZ.xtUdOlMiTwVfrX\n",
      "lTssTv.DQiQ?i! g\n"
     ]
    }
   ],
   "source": [
    "# If we want to determine the predicted charater we need to sample the output distribution (pick a value based on probability)\n",
    "sampled_indices = tf.random.categorical(pred, num_samples=1)\n",
    "\n",
    "# now we can reshape that array and convert all the integers to numbers to see the actual characters\n",
    "sampled_indices = np.reshape(sampled_indices, (1, -1))[0]\n",
    "predicted_chars = int_to_text(sampled_indices)\n",
    "\n",
    "# what the model predicted for training sequence 1\n",
    "print(predicted_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory where the checkpoints will be saved\n",
    "checkpoint_dir = \"./training_checkpoints\"\n",
    "\n",
    "# name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "172/172 [==============================] - 324s 2s/step - loss: 2.5540\n",
      "Epoch 2/2\n",
      "172/172 [==============================] - 326s 2s/step - loss: 1.8646\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(data, epochs=2, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.build(tf.TensorShape([1,None]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "    # evaluation step (generating text using the learned model)\n",
    "    \n",
    "    # number of characters to generate\n",
    "    num_generate = 800\n",
    "\n",
    "    # converting our start string to numbers (vectorizing)\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    # empty string to stor our results\n",
    "    text_generated = []\n",
    "\n",
    "    # low temperatures results in more preedictable text\n",
    "    # higher temperatures results in more surprisin text\n",
    "    # experiment to find the best setting\n",
    "    temperature = 1.0\n",
    "\n",
    "    # here batch size == 1\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        # remove batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        # using a categorical distribution to predict the character returned by the model\n",
    "        prediction = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        # pass the predicted character as the next input to the model\n",
    "        # along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (start_string + \"\".join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "romeop som.\n",
      "\n",
      "KIGG HENWARD II:\n",
      "Way ke dove Iblove at I see:\n",
      "If you what he sate that I not been like Mirriar!\n",
      "\n",
      "KA note on dine arther deed, not, our my manuesoug onjecen,\n",
      "Haw speny.\n",
      "\n",
      "KONCEY:\n",
      "Ay, if hal seed my parour wither my 'sweak,\n",
      "We habd he are stheid ag elore from bepone that sway me numble my and\n",
      "He chroof thould I kend mat, be mens\n",
      "Way I contigaling. What, it it is refe-boors?\n",
      "And tell ivuns han enger we senserd that with our tresp\n",
      "Palonst;\n",
      "Broodh vifferf him her this? I will fear, ere the withers?\n",
      "\n",
      "LADY CASPEY:\n",
      "\n",
      "LENCETIO:\n",
      "My loods, That, rush ictrare, pood good;\n",
      "He commpite-it mestice my pidse one herour\n",
      "Brase, sonciture us to the sier ond hir;\n",
      "buiringm in what, itl us, And which a frys it, Supit\n",
      "A marent epus him trorid:\n",
      "But but alper thr beares spower slowsh,\n",
      "havish thoubling poldom p\n"
     ]
    }
   ],
   "source": [
    "inp = input(\"Type a starting string: \")\n",
    "print(generate_text(model, inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
